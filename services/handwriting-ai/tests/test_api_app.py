from __future__ import annotations

import concurrent.futures as cf
import io
import time
from collections.abc import Callable, Sequence
from datetime import datetime
from pathlib import Path
from typing import Protocol, Self, runtime_checkable

import pytest
import torch.nn as nn
from fastapi.testclient import TestClient
from PIL import Image
from platform_core.errors import AppError, ErrorCode, HandwritingErrorCode
from platform_core.json_utils import JSONValue, dump_json_str, load_json_str
from torch import Tensor

from handwriting_ai.api.app import (
    _debug_invoke_reloader_start,
    _debug_invoke_reloader_stop,
    create_app,
)
from handwriting_ai.config import (
    AppConfig,
    DigitsConfig,
    SecurityConfig,
    Settings,
    limits_from_settings,
)
from handwriting_ai.inference.engine import InferenceEngine, LoadStateResult, TorchModel
from handwriting_ai.inference.manifest import ModelManifest
from handwriting_ai.inference.types import PredictOutput


class _DummyFuture:
    def __init__(self, result_fn: Callable[[float | None], PredictOutput] | None = None) -> None:
        self._fn = result_fn
        self._cancelled = False

    def result(self, timeout: float | None = None) -> PredictOutput:
        assert self._fn is not None
        return self._fn(timeout)

    def cancel(self) -> None:
        self._cancelled = True

    @property
    def cancelled(self) -> bool:
        return self._cancelled


@runtime_checkable
class _RespProto(Protocol):
    @property
    def status_code(self) -> int: ...

    @property
    def text(self) -> str: ...


def _resp_obj(r: _RespProto) -> dict[str, JSONValue]:
    obj: JSONValue = load_json_str(r.text)
    if not isinstance(obj, dict):
        raise AssertionError("response JSON is not an object")
    return obj


class _DummyModel:
    def eval(self) -> Self:
        return self

    def __call__(self, x: Tensor) -> Tensor:
        return x

    def load_state_dict(self, sd: dict[str, Tensor]) -> LoadStateResult:
        return LoadStateResult((), ())

    def train(self, mode: bool = True) -> Self:
        return self

    def state_dict(self) -> dict[str, Tensor]:
        return {}

    def parameters(self) -> Sequence[nn.Parameter]:
        return []


def _mk_settings(tmp: Path, *, api_key: str = "") -> Settings:
    app_cfg: AppConfig = {
        "data_root": tmp,
        "artifacts_root": tmp,
        "logs_root": tmp,
        "threads": 1,
        "port": 8081,
    }
    digits_cfg: DigitsConfig = {
        "model_dir": tmp / "models",
        "active_model": "active",
        "tta": False,
        "uncertain_threshold": 0.5,
        "max_image_mb": 1,
        "max_image_side_px": 1024,
        "predict_timeout_seconds": 1,
        "visualize_max_kb": 64,
        "retention_keep_runs": 1,
    }
    sec_cfg: SecurityConfig = {"api_key": api_key}
    return {"app": app_cfg, "digits": digits_cfg, "security": sec_cfg}


def _png_bytes(w: int = 28, h: int = 28, color: int = 0) -> bytes:
    img = Image.new("L", (w, h), color=color)
    buf = io.BytesIO()
    img.save(buf, format="PNG")
    return buf.getvalue()


def test_basic_routes_and_models(tmp_path: Path) -> None:
    settings = _mk_settings(tmp_path)
    engine = InferenceEngine(settings)
    app = create_app(settings=settings, engine_provider=lambda: engine)
    client = TestClient(app)

    # readyz not ready (no model/manifest) - returns 503 when degraded
    r1 = client.get("/readyz")
    body1 = _resp_obj(r1)
    assert r1.status_code == 503 and body1["status"] == "degraded"

    # Attach manifest and stub model to become ready
    man = ModelManifest(
        schema_version="v1.1",
        model_id="m1",
        arch="resnet18",
        n_classes=10,
        version="1.0",
        created_at=datetime.utcnow(),
        preprocess_hash="v1/grayscale+otsu+lcc+deskew{angle_conf}+center+resize28+mnistnorm",
        val_acc=0.9,
        temperature=1.0,
    )
    # Private attributes set only for readiness and metadata
    engine._manifest = man
    model: TorchModel = _DummyModel()
    engine._model = model

    r2 = client.get("/readyz")
    body2 = _resp_obj(r2)
    assert r2.status_code == 200 and body2["status"] == "ready"

    # /v1/models/active reflects manifest
    r3 = client.get("/v1/models/active")
    body3 = _resp_obj(r3)
    assert r3.status_code == 200 and body3["model_loaded"] is True
    assert body3["model_id"] == "m1"

    # Health endpoint
    assert client.get("/healthz").status_code == 200


def test_read_success_uncertain_and_visual(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    settings = _mk_settings(tmp_path)
    engine = InferenceEngine(settings)

    probs_tuple = tuple(0.1 for _ in range(10))
    out: PredictOutput = {"digit": 3, "confidence": 0.4, "probs": probs_tuple, "model_id": "m"}
    fut = _DummyFuture(lambda _t: out)

    def _submit_predict_stub(_: Tensor) -> _DummyFuture:
        return fut

    monkeypatch.setattr(engine, "submit_predict", _submit_predict_stub, raising=True)

    app = create_app(settings=settings, engine_provider=lambda: engine)
    client = TestClient(app)
    files = {"file": ("a.png", _png_bytes(), "image/png")}
    r = client.post("/v1/read?visualize=true", files=files)
    assert r.status_code == 200
    body = _resp_obj(r)
    assert body["digit"] == 3 and body["uncertain"] is True
    assert isinstance(body["visual_png_b64"], str) and len(body["visual_png_b64"]) > 0


def test_read_timeout_and_not_ready(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    settings = _mk_settings(tmp_path)
    engine = InferenceEngine(settings)

    # Timeout path
    def _raise_timeout(_: float | None) -> PredictOutput:
        raise cf.TimeoutError()

    fut_timeout = _DummyFuture(_raise_timeout)

    def _submit_timeout(_: Tensor) -> _DummyFuture:
        return fut_timeout

    monkeypatch.setattr(engine, "submit_predict", _submit_timeout, raising=True)
    app1 = create_app(settings=settings, engine_provider=lambda: engine)
    c1 = TestClient(app1)
    r1 = c1.post("/v1/read", files={"file": ("a.png", _png_bytes(), "image/png")})
    r1_body = _resp_obj(r1)
    assert r1.status_code == 504 and r1_body["code"] == ErrorCode.TIMEOUT

    # Not ready path
    def _raise_not_ready(_: float | None) -> PredictOutput:
        raise RuntimeError("Model not loaded")

    fut_not_ready = _DummyFuture(_raise_not_ready)

    def _submit_not_ready(_: Tensor) -> _DummyFuture:
        return fut_not_ready

    monkeypatch.setattr(engine, "submit_predict", _submit_not_ready, raising=True)
    app2 = create_app(settings=settings, engine_provider=lambda: engine)
    c2 = TestClient(app2)
    r2 = c2.post("/v1/read", files={"file": ("a.png", _png_bytes(), "image/png")})
    r2_body = _resp_obj(r2)
    assert r2.status_code == 503 and r2_body["code"] == ErrorCode.SERVICE_UNAVAILABLE


def test_read_validation_paths(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    settings = _mk_settings(tmp_path)
    engine = InferenceEngine(settings)
    # Successful future (not used when validation fails)
    probs_val = tuple(0.1 for _ in range(10))
    out: PredictOutput = {"digit": 1, "confidence": 0.9, "probs": probs_val, "model_id": "m"}

    def _return_out(_: float | None) -> PredictOutput:
        return out

    def _submit_success(_: Tensor) -> _DummyFuture:
        return _DummyFuture(_return_out)

    monkeypatch.setattr(engine, "submit_predict", _submit_success, raising=True)
    app = create_app(settings=settings, engine_provider=lambda: engine)
    client = TestClient(app)

    # Unsupported content type
    r0 = client.post("/v1/read", files={"file": ("x.txt", b"abc", "text/plain")})
    assert r0.status_code == 415

    # Extra unexpected multipart field
    files_extra = {"file": ("a.png", _png_bytes(), "image/png")}
    data_extra = {"junk": "x"}
    r1 = client.post("/v1/read", files=files_extra, data=data_extra)
    r1_body = _resp_obj(r1)
    assert r1.status_code == 400
    assert r1_body["code"] == HandwritingErrorCode.malformed_multipart.value

    # Multiple file parts not allowed
    files_multi = [
        ("file", ("a.png", _png_bytes(), "image/png")),
        ("file", ("b.png", _png_bytes(), "image/png")),
    ]
    r2 = client.post("/v1/read", files=files_multi)
    r2_body = _resp_obj(r2)
    assert r2.status_code == 400
    assert r2_body["code"] == HandwritingErrorCode.malformed_multipart.value

    # File body too large
    too_big = b"0" * (limits_from_settings(settings)["max_bytes"] + 1)
    r3 = client.post("/v1/read", files={"file": ("a.png", too_big, "image/png")})
    r3_body = _resp_obj(r3)
    assert r3.status_code == 413 and r3_body["code"] == ErrorCode.PAYLOAD_TOO_LARGE

    # Invalid image bytes
    r4 = client.post("/v1/read", files={"file": ("a.png", b"not-an-image", "image/png")})
    r4_body = _resp_obj(r4)
    assert r4.status_code == 400 and r4_body["code"] == HandwritingErrorCode.invalid_image.value

    # Image dimensions too large
    small_settings: Settings = {
        "app": settings["app"],
        "digits": {
            "model_dir": settings["digits"]["model_dir"],
            "active_model": settings["digits"]["active_model"],
            "tta": False,
            "uncertain_threshold": 0.5,
            "max_image_mb": 1,
            "max_image_side_px": 8,
            "predict_timeout_seconds": 1,
            "visualize_max_kb": 64,
            "retention_keep_runs": 1,
        },
        "security": settings["security"],
    }
    app2 = create_app(settings=small_settings, engine_provider=lambda: engine)
    c2 = TestClient(app2)
    r5 = c2.post("/v1/read", files={"file": ("a.png", _png_bytes(16, 16), "image/png")})
    r5_body = _resp_obj(r5)
    assert r5.status_code == 400 and r5_body["code"] == HandwritingErrorCode.bad_dimensions.value


def test_admin_upload_activate_and_errors(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    settings = _mk_settings(tmp_path)
    engine = InferenceEngine(settings)
    # Active model id matches upload so engine.try_load_active may be called
    d = settings["digits"]
    settings2: Settings = {
        "app": settings["app"],
        "digits": {
            "model_dir": d["model_dir"],
            "active_model": "m2",
            "tta": d["tta"],
            "uncertain_threshold": d["uncertain_threshold"],
            "max_image_mb": d["max_image_mb"],
            "max_image_side_px": d["max_image_side_px"],
            "predict_timeout_seconds": d["predict_timeout_seconds"],
            "visualize_max_kb": d["visualize_max_kb"],
            "retention_keep_runs": d["retention_keep_runs"],
        },
        "security": settings["security"],
    }
    app = create_app(settings=settings2, engine_provider=lambda: engine)
    client = TestClient(app)

    # Valid manifest JSON
    from handwriting_ai.preprocess import preprocess_signature

    man = {
        "schema_version": "v1.1",
        "model_id": "m2",
        "arch": "resnet18",
        "n_classes": 10,
        "version": "1.0",
        "created_at": datetime.utcnow().isoformat(),
        "preprocess_hash": preprocess_signature(),
        "val_acc": 0.9,
        "temperature": 1.0,
    }
    man_bytes = dump_json_str(man).encode("utf-8")

    # Patch loader and validator to succeed
    def __fake_load(_path: Path) -> dict[str, Tensor]:
        import torch

        return {"fc.weight": torch.zeros(1), "fc.bias": torch.zeros(1)}

    def __fake_validate(_sd: dict[str, Tensor], _arch: str, _n: int) -> None:
        pass

    monkeypatch.setattr(
        "handwriting_ai.api.routes.admin._engine_load_state_dict_file",
        __fake_load,
        raising=True,
    )
    monkeypatch.setattr(
        "handwriting_ai.api.routes.admin._engine_validate_state_dict",
        __fake_validate,
        raising=True,
    )

    files = {
        "manifest": ("manifest.json", man_bytes, "application/json"),
        "model": ("model.pt", b"weights", "application/octet-stream"),
    }
    data = {"model_id": "m2", "activate": "true"}
    r_ok = client.post("/v1/admin/models/upload", files=files, data=data)
    ok_body = _resp_obj(r_ok)
    assert r_ok.status_code == 200 and ok_body["ok"] is True

    # Invalid manifest JSON
    files_bad = {
        "manifest": ("manifest.json", b"not-json", "application/json"),
        "model": ("model.pt", b"weights", "application/octet-stream"),
    }
    r_bad = client.post("/v1/admin/models/upload", files=files_bad, data=data)
    bad_body = _resp_obj(r_bad)
    assert r_bad.status_code == 400
    assert bad_body["code"] == HandwritingErrorCode.preprocessing_failed.value

    # Activate False with empty model bytes â†’ invalid_model
    files_empty = {
        "manifest": ("manifest.json", man_bytes, "application/json"),
        "model": ("model.pt", b"", "application/octet-stream"),
    }
    data_empty = {"model_id": "m2", "activate": "false"}
    r_empty = client.post("/v1/admin/models/upload", files=files_empty, data=data_empty)
    empty_body = _resp_obj(r_empty)
    assert r_empty.status_code == 400
    assert empty_body["code"] == HandwritingErrorCode.invalid_model.value


def test_exception_handlers_and_api_key(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    # Require API key
    settings = _mk_settings(tmp_path, api_key="secret")
    engine = InferenceEngine(settings)
    probs_zeros = tuple(0.0 for _ in range(10))
    out: PredictOutput = {"digit": 0, "confidence": 1.0, "probs": probs_zeros, "model_id": "m"}

    def _return_out2(_: float | None) -> PredictOutput:
        return out

    def _submit_success2(_: Tensor) -> _DummyFuture:
        return _DummyFuture(_return_out2)

    monkeypatch.setattr(engine, "submit_predict", _submit_success2, raising=True)
    app = create_app(settings=settings, engine_provider=lambda: engine)

    def _boom_app() -> None:
        raise AppError(HandwritingErrorCode.invalid_image, "bad", 400)

    def _boom_generic() -> None:
        raise RuntimeError("fail")

    app.add_api_route("/boom-app", _boom_app, methods=["GET"])
    app.add_api_route("/boom-generic", _boom_generic, methods=["GET"])

    client = TestClient(app, raise_server_exceptions=False)
    files = {"file": ("a.png", _png_bytes(), "image/png")}
    # Unauthenticated should fail
    r_unauth = client.post("/v1/read", files=files)
    assert r_unauth.status_code == 401
    # Authenticated works
    r_auth = client.post("/v1/read", files=files, headers={"X-API-Key": "secret"})
    assert r_auth.status_code == 200

    e1 = client.get("/boom-app")
    e1_body = _resp_obj(e1)
    assert e1.status_code == 400 and e1_body["code"] == HandwritingErrorCode.invalid_image.value
    e2 = client.get("/boom-generic")
    e2_body = _resp_obj(e2)
    assert e2.status_code == 500 and e2_body["code"] == ErrorCode.INTERNAL_ERROR


def test_optional_reloader_handlers(tmp_path: Path) -> None:
    settings = _mk_settings(tmp_path)
    engine = InferenceEngine(settings)
    app = create_app(
        settings=settings, engine_provider=lambda: engine, reload_interval_seconds=0.05
    )
    # Manually invoke start/stop handlers
    _debug_invoke_reloader_start(app)
    time.sleep(0.12)
    _debug_invoke_reloader_stop(app)
    # We cannot introspect private state; ensure the app still responds
    client = TestClient(app)
    assert client.get("/healthz").status_code == 200

    # Interval <= 0: handlers not registered, calls should be no-op
    app2 = create_app(
        settings=settings, engine_provider=lambda: engine, reload_interval_seconds=0.0
    )
    _debug_invoke_reloader_start(app2)
    _debug_invoke_reloader_stop(app2)
